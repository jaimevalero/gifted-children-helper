# Model para debug en local con ollama
OLLAMA_MODEL_NAME="ollama/llama3.1:8b"
OLLAMA_API_BASE="http://127.0.0.1:11434"
OLLAMA_MODEL_AUX_EMBED_NAME="ollama/llama3.1:8b"

#OLLAMA_MODEL_AUX_EMBED_NAME="ollama/qwen2.5:14b-8k"

DEBUG=1
API_TOKEN="<replace for your token>"
#OLLAMA_MODEL_NAME="ollama/llama3.1-16k:8b"
#OLLAMA_API_BASE="http://127.0.0.1:11434"


#OLLAMA_MODEL_NAME=ollama/exaone3.5:7.8b
#OLLAMA_API_BASE=http://127.0.0.1:11434

#OLLAMA_MODEL_NAME="ollama/qwen2.5-coder:14b" # Este funciona fatal
#OLLAMA_API_BASE=http://127.0.0.1:11434
#OLLAMA_MODEL_NAME=ollama/llama3.3:70b

# Este es el mejor para producción, probablemente
#OLLAMA_MODEL_NAME=ollama/qwen2.5:72b
#OLLAMA_API_BASE = http://epg-ollama.hi.inet:11434

# OLLAMA_MODEL_NAME=ollama/llama3.3:70b
# OLLAMA_API_BASE = http://epg-ollama.hi.inet:11434

# Este está bien, aunque claude opina que es mejorable
# OLLAMA_MODEL_NAME=ollama/qwen2.5-8k:14b 
# OLLAMA_API_BASE=http://127.0.0.1:11434
# OLLAMA_MODEL_AUX_EMBED_NAME=ollama/qwen2.5-8k:14b

 # Da error LiteLLM de aPi FALLA
# OLLAMA_MODEL_NAME=ollama/llama3.3:70b-4k
# OLLAMA_API_BASE = http://epg-ollama.hi.inet:11434
# OLLAMA_MODEL_AUX_EMBED_NAME=ollama/llama3.1:8b-8k

# Bunciona bien, junto con el de 17k son los mejores
# OLLAMA_MODEL_NAME=ollama/qwen2.5:32b-8k
# OLLAMA_API_BASE=http://epg-ollama.hi.inet:11434
# OLLAMA_MODEL_AUX_EMBED_NAME=ollama/qwen2.5:32b-8k

# No consigo hacerlo tirar.
# OLLAMA_MODEL_NAME=ollama/qwen2.5:72b-8k
# OLLAMA_API_BASE=http://epg-ollama.hi.inet:11434
# OLLAMA_MODEL_AUX_EMBED_NAME=ollama/llama3.1:8b-8k


# Disable Telemetry
OTEL_SDK_DISABLED=true

# Do not change, model for embedding
# EMBED_MODEL = mxbai-embed-large
EMBED_MODEL = snowflake-arctic-embed2
DEFAULT_REQUEST_TIMEOUT = 3600
DEFAULT_CONTEXT_WINDOW = 4096